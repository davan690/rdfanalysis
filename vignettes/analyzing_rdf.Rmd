---
title: "Analyzing Researchers Degrees of Freedom: A Case Study"
author: "Joachim Gassen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
set.seed(42)
```

## Introduction

The idea of the in-development `rdfanalysis` package is to provide a coding environment that allows researchers to systematically document and analyze the degrees of freedom in their research designs. The notion "researcher degrees of freedom" has been introduced by [Simmons et al., (Psyc Science, 2011)](https://journals.sagepub.com/doi/pdf/10.1177/0956797611417632). In their own words (p. 1359): 

> The culprit is a construct we refer to as *researcher degrees of freedom*.
> In the course of collecting and analyzing data,
> researchers have many decisions to make: Should more data
> be collected? Should some observations be excluded? Which
> conditions should be combined and which ones compared?
> Which control variables should be considered? 
> Should specific measures be combined or transformed or both?

This vignette will use a case to guide you through the process of how to use the package for an analysis that make its inherent researcher degrees of freedom transparent.
The outcome of this activity will be similar to the multiverse analysis approach suggested by [Steeger et al. (Persp on Psych Sci 2016)](https://journals.sagepub.com/doi/abs/10.1177/1745691616658637).

## The Case

As case I assess the impact of real Gross Domestic Product (GDP) per capita on life expectancy. This association has become known as the [Preston Curve](https://en.wikipedia.org/wiki/Preston_curve) ([Preston, Pop Studies, 1975](https://www.tandfonline.com/doi/abs/10.1080/00324728.1975.10410201)). While the analysis it not meant to contribute to the question whether there is a causal link between these two constructs, I design the analysis such that identifying assumptions for such a causal interpretation of its findings are explicit.

## Step 1: Define your research design by a series of functions

Every research design can be implemented as a series of functions where each function reads the output of the previous function and produces output for the next function.
The first function reads or generates data, the last function generates results. In principle, the `rdfanalysis` package is agnostic about the data structure that constitutes a result. In most cases, the result will take the form of one or several confidence intervals.

To assess the impact of GDP per capita on life expectancy, our case design follows the following steps

- Read data
- Define variables
- Select variables for model
- Treat extreme observations
- Estimate model

Each step will be implemented via a function. To set up such a functional structure, you use the command `define_design()`. This function creates a series of function templates in a directory of your choice. Existing files are not over-written. You can choose whether you want your functions all in one R file or whether you want to go with separtate files. I am going with seperate files here.

``` {r define_design, results = "hide"}
library(rdfanalysis)
library(tidyverse)
library(ExPanDaR)
library(knitr)

design <- define_design(steps = c("read_data", 
                                  "define_vars", 
                                  "select_vars", 
                                  "treat_extreme_obs", 
                                  "est_model"),
                        rel_dir = "case_study_code")
```

## Step 2: Implement design steps

When you take a look at the newly created directory, you will see a set of R files. Each file contains a template like this:

``` {r template}
step_name <- function(input = NULL, choice = NULL) {
  # This is a template for a design step
  # to be used by the rdfanalysis package
  # See https://joachim-gassen.github.io/rdfanalysis
  # for further information on how to use this template
  # and the package.

  # Feel free to delete any/all lines containing comments but leave
  # the line containing ("Analysis code starts below") unchanged.

  # Provide your documentation in the two variables below.
  # They will be used by prepare_design_documentation()

  step_description <- c(
    "## [Step Title]",
    "### Content",
    "",
    "[Explain what the step does]"
  )
  choice_description <- c(
    "### Choice",
    "",
    "`[name of discrete choice]`: A character value containing one of the",
    "following values:",
    "",
    "- `[valid_value 1]`: [What this choice does]",
    "- `[valid_value n]`: [What that choice does]",
    "",
    "`[name of continous choice]`: [Explain valid range for continous choice",
    "and what it does]"
  )

  # Specify your valid choices below. Format will be checked by test_design()
  # for consistency

  choice_type <- list(
    list(name = "[name of discrete choice]",
         type = "character",
         valid_values = c("[valid_value 1]", "[valid_value n]"),
         # weights are not required but they need to add up to one if
         # they are provided. Adjust the example below
         weights = c(0.8, 0.2)),
    list(name = "[name of continous choice]",
         type = "double",
         valid_min = 0, valid_max = 0.1,
         # weights for continous choices are provided by a sample of choices.
         # Could be just one value. Adjust the example below
         weight_sample = c(0, rep(0.01, 4), rep(0.05, 4)))
  )

  # You should not need to change anything in the following code section

  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)

  # Leave the following comment untouched. It is being used by
  # prepare_design_documentation() to identify the part of the step
  # that should be included in the documentation.

  # ___ Analysis code starts below ___

  # Here you need to add your code.

  # If this is the first step of your design, you can access input data via the
  # input parameter. If it is a subsequent step, you can access input data
  # via input$data and the protocol leading to this step by input$protocol.

  # Make sure that you address all choices identified above.

  # In the return block below, you need to replace the placeholder with the
  # variable that contains the data that you want to return as output
  # to the next step or as a result if this is the last step.

  return(list(
    data = "[variable containing your output data structure here]",
    protocol = list(choice)
  ))
}
```

The task is now to implement the code for each step. To do so, you should first set `step_description` and `choice_description`. Each variable takes a vector of strings that will be concatenated to RMarkdown code. Use these variables to explain what the steps does and which choices it contains. Each choice establishes a researcher degree of freedom.

As an example: When we import the raw data, we can decide whether we require each country-year observation to have non-missing data for all variables. To implement this discrete choice, you document it in `choice_description` and specify it in `choice_type`. The `weight` parameter can be used to provide the choice (or weighting of choices) that you *a priori* assume to be supported by theory. The final function `read_data()` is provided below. It simply reads a csv file containing data, limits the sample to the required variables and deletes cases with missing data if the choice `na.omit` is set to `"yes"`.

``` {r read_data}
read_data <- function(input = NULL, choice = NULL) {
  step_description <- c(
    "## Read data",
    "### Content",
    "",
    "Reads country year world bank data CSV file and generates raw sample"
  )
  choice_description <- c(
    "### Choice",
    "",
    "`na.omit`: A character value containing one of the",
    "following values:",
    "",
    "- `yes`: All observations with missing data are excluded",
    "- `no`: All observations with missing data are included"
  )

  choice_type <- list(
    list(name = "na.omit",
         type = "character",
         valid_values = c("yes", "no"),
         weights = c(1, 0))
  )
  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)
  # ___ Analysis code starts below ___

  df <- read_csv(input, col_types = cols()) %>%
    select(country, year,
           lifeexpectancy, gdp_capita,
           resdevelop_gdp, unemployment) 
  
  if(choice == "yes") df <- df %>%
    na.omit()

  return(list(
    data = df,
    protocol = list(choice)
  ))
}
```

For the other steps you follow this approach. For our case study, I implement the following choices:

- `define_vars()`: `log_gdp_capita` determines whether the main variable of interest is used in logged specification or not.
- `select_vars()`: `idvs` identifies the independent variables that you want to include in the regression model
- `treat_extreme_obs()`: `outlier_tment_style` indicates whether you want to use truncating or winsorizing to limit the influence of extreme observations. `outlier_cutoff` holds the percentile cutoff. It can be within the range [0, 0.1] (0 to 10 %).
- `est_model()`: `feffects` reports which fixed effects are used (none, country, year or both) and `cluster` indicates how the standard errors should be clustered (not, by country, by year, or two-way by country and year).

All these choices are rather standard choices in research designs based on observational data. Taken together and assuming that you want to generate outlier cases for each percentile from 0 to 10, they generate $2 * 2 * 4 * 2 * 11 * 4 * 4 = 5,632$ researcher degrees of freedom! 

Below you will find all four functions. Copy and paste them into the template files to generate your research design.

``` {r functions, eval = FALSE}
define_vars <- function(input = NULL, choice = NULL) {
  step_description <- c(
    "## Define the variables to be used in the analysis",
    "### Content",
    "",
    "Based on the raw data, define the measures to be used in the analysis."
  )
  choice_description <- c(
    "### Choice",
    "",
    "`log_gdp_capita`: A character value containing one of the",
    "following values:",
    "",
    "- `yes`: Take the natural logarithm",
    "- `no`: Use as in (USD value)"
  )
  choice_type <- list(
    list(name = "log_gdp_capita",
         type = "character",
         valid_values = c("yes", "no"),
         weights = c(1, 0))
  )
  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)
  # ___ Analysis code starts below ___
  
  df <- input$data
  
  if (choice == "yes")
    df$gdp_capita <- log(df$gdp_capita)
  
  protocol <- input$protocol
  protocol[[length(protocol) + 1]] <- choice
  
  return(list(
    data = df,
    protocol = list(choice)
  ))
}


select_vars <- function(input = NULL, choice = NULL) {
  step_description <- c(
    "## Identify the variables for analysis",
    "### Content",
    "",
    "Select the variables that you want to use in the regression analysis."
  )
  choice_description <- c(
    "### Choice",
    "",
    "`idvs`: A character value containing one of the",
    "following values:",
    "",
    "- `gdp_only`: GDP per capita only",
    "- `gdp_rd`: GDP per capita and research and development over GDP",
    "- `gdp_ue`: GDP per capita and unemployment",
    "- `full`: Full model including all three variables"
  )
  choice_type <- list(
    list(name = "idvs",
         type = "character",
         valid_values = c("gdp_only", "gdp_rd", "gdp_ue", "full"),
         weights = c(0, 0, 0, 1))
    )
  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)
  # ___ Analysis code starts below ___

  df <- switch (choice[[1]],
    "gdp_only" = input$data %>% select(-resdevelop_gdp, -unemployment),
    "gdp_rd" = input$data %>% select(-unemployment),
    "gdp_ue" = input$data %>% select(-resdevelop_gdp),
    "full" = input$data
  )

  protocol <- input$protocol
  protocol[[length(protocol) + 1]] <-  choice
  return(list(
    data = df,
    protocol = protocol
  ))
}

treat_extreme_obs <- function(input = NULL, choice = NULL) {
  step_description <- c(
    "## Select the type of outlier treatment",
    "### Content",
    "",
    "Select how you want to treat extreme observations. They can be winsorized or truncated",
    "to a given percentile of the data."
  )
  choice_description <- c(
    "### Choice",
    "",
    "A list containing a character value `outlier_tment_style` and a numerical value `outlier_cutoff`.",
    "`outlier_tment_style` may take one of the following values:",
    "",
    "- `win`: Winsorization",
    "- `trunc`: Truncation",
    "",
    "`outlier_cutoff` sets the cut-off percentile for the outlier treatment",
    "and may take any value within [0, 0.10]"
  )
  choice_type <- list(
    list(name = "outlier_tment_style",
         type = "character",
         valid_values = c("win", "trunc"),
         weights = c(0.5, 0.5),
    list(name = "outlier_cutoff",
         type = "double",
         valid_min = 0, valid_max = 0.1,
         weight_sample = c(0, 0, rep(0.01, 4), rep(0.05, 4)))
  )
  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)
  # ___ Analysis code starts below ___

  if (choice[[2]] == 0) return(
    list(
      data = input$data,
      protocol = input$protocol[[length(input$protocol) + 1]] <- choice
    ))
  switch(choice[[1]],
         "win" = input$data %>%
           treat_outliers(percentile = choice[[2]]) -> df,
         "trunc" = input$data %>%
           treat_outliers(percentile = choice[[2]],
                          truncate = TRUE) -> df)

  protocol <- input$protocol
  protocol[[length(protocol) + 1]] <-  choice
  return(list(
    data = df,
    protocol = protocol
  ))
}

est_model <- function(input = NULL, choice = NULL) {
  step_description <- c(
    "## Estimates the model",
    "### Content",
    "",
    "Uses a multiple regression setup to generate an estimate and",
    "a confidence interval for the effect of GDP per capita on life",
    "expectancy."
  )
  choice_description <- c(
    "### Choice",
    "",
    "A list containing two character values: `cluster` and `feffect`.",
    "`cluster` defines the clustering of standard errors and",
    "`feffect` defines the fixed effect structure of the model.",
    "Each value may take one of the following values:",
    "",
    "- `none`, `ctry`, `year` or `ctryyear`"
  )
  choice_type <- list(
    list(name = "feffect",
         type = "character",
         valid_values = c("none", "country", "year", "ctryyear"),
         weights = c(0, 0, 0, 1)),
    list(name = "cluster",
         type = "character",
         valid_values = c("none", "country", "year", "ctryyear"),
         weights = c(0, 0, 0, 1))
  )
  if (is.null(choice)) return(list(
    step_description = step_description,
    choice_description = choice_description,
    choice_type = choice_type
  )) else check_choice(choice, choice_type)

  # ___ Analysis code starts below ___

  f <- paste0("lifeexpectancy ~ ",
              paste(colnames(input$data)[4:length(input$data)], collapse = " + "))

  if (choice[[1]] == "country" || choice[[1]] == "year") f <- paste0(f, "| ", choice[[1]])
  else if (choice[[1]] == "ctryyear") f <- paste0(f, " | country + year")

  if (choice[[2]] == "none") form <- as.formula(f)
  else if (choice[[1]] == "none") f <- paste0(f, " | 0 ")

  if (choice[[2]] == "country" || choice[[2]] == "year") f <- paste0(f, "| 0 | ", choice[[2]])
  else f <- paste0(f, "| 0 | country + year ")

  if (choice[[2]] != "none") form <- as.formula(f)

  mod <- lfe::felm(form, input$data)

  l <- list(
    est = mod$coefficients[row.names(mod$coefficients) == 'gdp_capita'],
    lb = confint(mod)[row.names(mod$coefficients) == 'gdp_capita', 1],
    ub = confint(mod)[row.names(mod$coefficients) == 'gdp_capita', 2]
  )

  protocol <- input$protocol
  protocol[[length(protocol) + 1]] <-  choice
  return(list(
    data = l,
    protocol = protocol,
    model = mod
  ))
}
```

## Step 3: Source, test and document design

After implementing the design you need to source it. In addition, you can test the design code for input parameter consistency at this stage.

``` {r source_design}
source_design(design, rel_dir = "case_study_code")
test_design(design, reporter = "minimal")
```

Next, the command `prepare_design_documentation()` produces a PDF document based on 
the `step_description` and `choice_description` that you provided for each step. By default,
it also includes the "pay load" code for each design step.

In addition, the command `prepare_design_flow_chart()` produces a visualization of your design steps and of their choices.

``` {r vis_design_steps, fig.width = 8, fig.height = 12, fig.align = "center"}
prepare_design_flow_chart(design)
```

## Step 4 (optional but encouraged): Simulate data for additional testing and power analysis

While not strictly necessary, a very useful next step is to simulate your raw data. Simulating your data early in the project can be very helpful for various reasons:

* It allows you to test your design steps without requiring you to work with your real data, which would strictly speaking constitute a breach of the "don't mix exploration with inference" rule.
* You can simulate your expected effect and verify that your design identifies it correctly.
* You can assess the power of your design.

To demonstrate these steps, I use a very quick'n'dirty simulation approach. 
It creates a country year panel and simulates data that are roughly comparable 
to real data. Using the natural log of `gdp_capita` as cause,
It applies an effect of magnitude `es` on `life_expectancy` and
returns a path to a temporary CSV file containing the data.

```{r simulation code}
sim_data <- function(countries = 75, es = 1, years = 15) {
  n <- countries * years
  df <- expand.grid(country = as.factor(1:countries), 
                    year = as.ordered((2016 - years + 1):2016))

  mm_gdp_capita <- 18000
  sd_gdp_capita <- 20000
  location <- log(mm_gdp_capita^2 / sqrt(sd_gdp_capita^2 + mm_gdp_capita^2))
  shape <- sqrt(log(1 + (sd_gdp_capita^2 / mm_gdp_capita^2)))

  base_gdp_capita <- rlnorm(countries, location, shape)
  base_unemp <- rbeta(countries, 2, 20)*100
  base_rd_gdp <- rbeta(countries, 1.5, 150)*100
  base_le <- rnorm(countries, 70, 10)
  base_le[base_le > 70] <- 70 + 0.6 * (base_le[base_le > 70] - 70)

  for (yr in (2016 - years + 1):2016) {
    if (yr == 2016 - years + 1) df$gdp_capita[df$year == yr] <- base_gdp_capita
    else df$gdp_capita[df$year == yr] <- pmax(df$gdp_capita[df$year == yr - 1] +
                                            rnorm(countries, 0, 0.01 * sd_gdp_capita), 100) *
        (1 + rnorm(countries, 0.02, 0.02))
  }
  df$resdevelop_gdp <- pmax(rep(base_rd_gdp, years) +
                              0.1 * (log(df$gdp_capita) - mean(log(df$gdp_capita))) +
                              rnorm(n, 0.10, 0.05), 0)
  df$unemployment <- pmax(rep(base_unemp, years) - 
                            0.2 * (log(df$gdp_capita) - 
                                     mean(log(df$gdp_capita))) + 
                            rnorm(n, 0.2, 0.2), 0)
  df$lifeexpectancy <- rep(base_le, years) + 
    df$resdevelop_gdp - 
    0.5 * df$unemployment +
    es * (log(df$gdp_capita) - mean(log(df$gdp_capita))) + 
    rnorm(n, 0, 5)

  df <- df[, c("country", "year", "lifeexpectancy", 
               "gdp_capita", "resdevelop_gdp", "unemployment")]
  tfile <- tempfile("simdata", fileext = c(".csv"))
  write_csv(df, tfile)
  tfile
}
```

Using this code we can now test whether our design steps are able to process the data.

```{r test_code}
test_design(design, input = sim_data(), reporter = "minimal")
```

This looks good. Now let's see whether the design generates an effect estimate that is consistent with the effect of 10 that we simulated. For that, we pick the protocol that is consistent with the simulation, meaning that

- we use the log transformed value of `gdp_capita` as the independent variable,
- use the full model since `sim_data()` incorporated effects of `unemployment` and `resdevelop_gdp`
- and we do not treat outliers.

We use the function `simulate_design_power()` for this step. This function runs a provided protocol on simulated data, with specified sample and effect size. The sample size is given in cross-sectional units here. The function returns a data frame containing the results for each simulation run. The histogram of the point estimates allows us to assess the consistency of our design.

``` {r check_design_consist, fig.align = "center"}
power_df <- simulate_design_power(design, protocol = list(na_omit = "no", 
                                                          log_gdp_capita = "yes", 
                                                          idvs = "full",
                                                          list(
                                                            outlier_tment_style = "win",
                                                            outlier_cutoff = 0
                                                          ),
                                                          list(
                                                            fecffect = "ctryyear",
                                                            cluster = "country"
                                                          )), 
                                  input_sim_func = sim_data, 
                                  range_n = 75,
                                  effect_size = 10.0)

ggplot(power_df, aes(x = est)) + geom_density(color = NA, fill = "lightblue")
```

This looks OK. Now: How about the power? Let us assume that we exect a 10% increase in GDP per capita to increase life expectancy by a year. How likely is that we will observe this effect in the data? The coefficient for the log transformed independent variable in that case would be $1 / ln(1.1) \approx `r round(1/log(1.1), 2)`$. Let's see whether we have enough power to detect such an effect.

``` {r check_power, fig.align = "center", cache = TRUE}
power_df <- simulate_design_power(design, protocol = list(na_omit = "no", 
                                                          log_gdp_capita = "yes", 
                                                          idvs = "full",
                                                          list(
                                                            outlier_tment_style = "win",
                                                            outlier_cutoff = 0
                                                          ),
                                                          list(
                                                            fecffect = "ctryyear",
                                                            cluster = "country"
                                                          )), 
                                  input_sim_func = sim_data, 
                                  range_n = seq(10, 100, 10),
                                  effect_size = 1/log(1.1))

ggplot(power_df, aes(x = n, y = est)) + geom_point(color = "lightblue", position = position_dodge(width = 0.5))
```

As `n`, the number of cross-sectional units (countries) in our simulation, increases, the distribution of our point estimates for our simulated effect narrows and centers on the simulated effect size. This is good. What is the number of countries that are needed, assuming 15 observations per country, to reach a power of $0.8$?

```{r power_graph, fig.align = "center"}
power_df %>%
  group_by(n) %>%
  summarize(power = sum(lb > 0)/n()) %>%
  ggplot(aes(x = n, y = power)) + geom_line(color = "lightblue", size = 2)
```

Roughly 30 countries, yielding 450 observations. The world bank data will have more observations than that. So, power will not be an issue in our analysis.

We have now verified that our design succeeds in identifying a simulated effect without bias and sufficient power. Time to run it on real data.

## Step 5: Estimating the effect

As we provided weights for each design step choice, we can now estimate the effect by applying these weights on our design. This is done by first running `exhaust_design(..., weight = TRUE)` based on the actual data and by then using the return value from this function call as input to `calculate_weighted_estimate()`.

``` {r estimate_weighted_effect}

weighted_ests <- exhaust_design(design, "https://joachim-gassen.github.io/data/wb_condensed.csv",
                                weight = TRUE)
calculate_weighted_estimate(weighted_ests, "est", "lb", "ub")
```

We can also assess the results per protocol. 

``` {r table_weighted_results}
kable(weighted_ests)
```

The estimated effect sizes are much smaller than simulated and significant at the conventional level of $5 \%$ in two out of six cases. You can also run a single protocol to analyze the full model results.

``` {r single_protocol}
res <-
  read_data("https://joachim-gassen.github.io/data/wb_condensed.csv", "yes") %>%
  define_vars("yes") %>%
  select_vars("full") %>%
  treat_extreme_obs(list("win", 0.01)) %>%
  est_model(list("ctryyear", "ctryyear"))

summary(res$model)
```

Based on the selected model one would conclude that the additional explanatory power of our independent variables in explaining life expecvtancy is not overwhelming once one allows for country and year fixed effects.

How robust is our effect estimate across our documented researcher degrees of freedom? Time to exhaust our design across all available design choices.


## Step 6: Assessing the robustness of the effect

We will now exhaust all design choices and estimate our effect for each feasible combination of choices. As these combine to $5,632$ protocols, running the code below will take a while. This is why we load the data that is generated by the commented lines from the disk.

``` {r exhaust_design}
# ests <- exhaust_design(design, "https://joachim-gassen.github.io/data/wb_condensed.csv")
# save(ests, file = "case_study_ests.RData")

load("case_study_ests.RData")
```

`plot_rdf_estimate_density()` provides a quick visual of the range of estimates across all researcher degrees of freedom. 

``` {r estimate_density, fig.width = 8, fig.height = 6, fig.align = "center"}
plot_rdf_estimate_density(ests, "est", "lb", "ub", color = "lightblue")
```

It becomes apparent that the estimated effect of GDP per capita varies widely across different research protocols. To explore this variation, we next display coefficients relative to discrete choices. We start with the specification of the main variable of interest. To be able to assess the precise range of our estimates, we use histogram-based ridge lines instead of densities.

``` {r estimate_density_by_log_gdp_capita, fig.width = 8, fig.height = 6, fig.align = "center"}
plot_rdf_ridges_by_dchoice(ests, "est", "log_gdp_capita", hist = TRUE, scale = 0.9, fill = "lightblue", color = NA)
```

Not surprisingly, whether or not we use a logged version of the independent variable has a substantial effect on the estimated coefficients. Specifications using a non-logged independent variable are mostly centured around zero. Given that the association of `gdp_capita` and `lifeexpectancy` documented by the Preston curve is highly non-linear and that theoretically it seems reasonable to assume that percentage change and not absolute change in GDP per capita drive changes in life expectancy, we will now focus on specifications that use the log variant of the independent variable. 

``` {r estimate_density_by_feffect, fig.width = 8, fig.height = 6, fig.align = "center"}
log_ests <- ests %>%
  filter(log_gdp_capita == "yes")

plot_rdf_ridges_by_dchoice(log_ests, "est", "feffect", hist = TRUE, scale = 0.9, fill = "lightblue", color = NA)
```

The influence of the applied fixed effect structure on the coefficient of interest is striking. A rigid set of country and year fixed effets controls for unobserved time-invariant cross-sectional heterogeneity and for unobserved cross-sectionally-invariant time trends. The former can be explained by, e.g., regional determinants like climate influencing life expectancy and the latter by developments in medical services and other social aspects that materialize across time and are constant across countries. As both are likely to be influential on theoretical grounds, we limit the analysis now on protocols that employ country and firm fixed effects and that use the natural log of GDP per capita as the main variable of interest. Also, we limit the analysis to models that estimate confidence intervals based on two-way clustered standard errors for the same reason.

``` {r estimate_density_by_idvs, fig.width = 8, fig.height = 6, fig.align = "center"}
rel_ests <- log_ests %>%
  filter(feffect == "ctryyear" & 
           cluster == "ctryyear")

plot_rdf_ridges_by_dchoice(rel_ests, "est", "idvs", hist = TRUE, scale = 0.9, fill = "lightblue", color = NA)
```

Hmm... While most of the protocols generate an estimate roughly in the range [0.5, 1.5], there are some estimates that are consistently larger. Are those driven by the sample selection step?

``` {r estimate_density_by_idvs_na, fig.width = 8, fig.height = 6, fig.align = "center"}
rel_ests %>%
  filter(na.omit == "yes") %>%
  plot_rdf_ridges_by_dchoice("est", "idvs", hist = TRUE, scale = 0.9, fill = "lightblue", color = NA)
```

To some extent. When the outlier treatment is being applied on a smaller sample it seems to drive the coefficient higher. Other then that the control variables seem to have little influence on the main coefficient. 

Finally, let's have a look at the outlier treatment.

``` {r estimate_density_by_outlier, fig.width = 8, fig.height = 6, fig.align = "center"}
rel_ests %>%
  filter(na.omit == "yes",
         idvs == "full") %>%
  plot_rdf_estimates_by_choice("est", "lb", "ub", "outlier_tment_style", 
                               color = "outlier_cutoff", order = "outlier_cutoff", width = 0.8)
```


## To be continued ...

This is work in process. Come back for more ;-)

